{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gae-ddong/mlteamproject/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaQi2LTwPNco",
        "outputId": "796c4da6-b5e9-47ba-856d-7ba7133edbc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O_T_aU6PdD1",
        "outputId": "b33b4151-cd06-470f-dbce-8d35b6feefde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1G6SgIRKCYt5sUkkaCAiAmdP0_hZA-rFs/data\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3zfoxPtPjlS",
        "outputId": "e5f4fb8c-094f-4e8b-e422-ac62104173b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mblurr_cut_png\u001b[0m/   \u001b[01;34mFinal_data\u001b[0m/  \u001b[01;34mimages_cut2_png\u001b[0m/  \u001b[01;34mimages_jpg\u001b[0m/  \u001b[01;34mincipient\u001b[0m/  \u001b[01;34mno\u001b[0m/\n",
            "\u001b[01;34mcut_resize_png\u001b[0m/  \u001b[01;34mfirst_data\u001b[0m/  \u001b[01;34mimages_gray_jpg\u001b[0m/  \u001b[01;34mimages_png\u001b[0m/  \u001b[01;34mmature\u001b[0m/     \u001b[01;34moverripe\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tMTJprqafPP",
        "outputId": "ac139f98-8c60-4bff-f273-1973352e242c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEDGN4ACa6Lb",
        "outputId": "202237ee-eb23-402c-c8ac-919e84e49c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjiyV8lUbFwo",
        "outputId": "73940d75-8fe4-45cf-e6bc-0e79cb944270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vit-keras in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit-keras) (1.11.4)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from vit-keras) (0.28.3)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit-keras) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "pip install vit-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE16NJ5TPlN_",
        "outputId": "81215b6e-a4b2-4b15-f76a-c7e9009de9f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pickle\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow_addons as tfa\n",
        "from vit_keras import vit\n",
        "from vit_keras import visualize\n",
        "\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "34-qwn4kPr92"
      },
      "outputs": [],
      "source": [
        "images_final_data_path = '/content/drive/My Drive/data/Final_data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fJHViDgyXvON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5802420-08c0-4117-fe27-dc8e143529a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_cam.jpg  incipient  mature  no  overripe\n"
          ]
        }
      ],
      "source": [
        "os.chdir(images_final_data_path)\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DVt1LWReYSTP"
      },
      "outputs": [],
      "source": [
        "image_path = images_final_data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNicedTHaWLX"
      },
      "source": [
        "Pickle load to variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxkl8HkPXz1y",
        "outputId": "377c5336-c8a7-4861-e792-54a234fe6448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overripe :  2000\n",
            "no :  2000\n",
            "mature :  2000\n",
            "incipient :  2000\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.join(image_path, 'overripe'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     overripe = pickle.load(f)\n",
        "     print('overripe : ', len(overripe))\n",
        "os.chdir(os.path.join(image_path, 'no'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     no = pickle.load(f)\n",
        "     print('no : ', len(no))\n",
        "\n",
        "os.chdir(os.path.join(image_path, 'mature'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     mature = pickle.load(f)\n",
        "     print('mature : ', len(mature))\n",
        "\n",
        "os.chdir(os.path.join(image_path, 'incipient'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     incipient = pickle.load(f)\n",
        "     print('incipient : ', len(incipient))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIY8mkCtabv1"
      },
      "source": [
        "Dictionary to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yxMGqb3NYVh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad60fd8-a84e-4247-d7fc-65769ff7bed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overripe_data shape: (2000, 224, 224, 3)\n",
            "no_data shape: (2000, 224, 224, 3)\n",
            "mature_data shape: (2000, 224, 224, 3)\n",
            "incipient_data shape: (2000, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "# overripe의 모든 사진에 대해 반복\n",
        "overripe_data_list = []\n",
        "\n",
        "for filename, data in overripe.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    overripe_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "overripe_data = np.array(overripe_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"overripe_data shape:\", overripe_data.shape)\n",
        "no_data_list = []\n",
        "\n",
        "for filename, data in no.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    no_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "no_data = np.array(no_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"no_data shape:\", no_data.shape)\n",
        "\n",
        "mature_data_list = []\n",
        "\n",
        "for filename, data in mature.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    mature_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "mature_data = np.array(mature_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"mature_data shape:\", mature_data.shape)\n",
        "\n",
        "incipient_data_list = []\n",
        "\n",
        "for filename, data in incipient.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    incipient_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "incipient_data = np.array(incipient_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"incipient_data shape:\", incipient_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QXUwoi-akaH"
      },
      "source": [
        "Train Val Test split (0.75:0.15:0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsXIqEfraDw-",
        "outputId": "4de0a993-e7d4-4220-84c0-d7290721fa52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5600, 224, 224, 3)\n",
            "(1200, 224, 224, 3)\n",
            "(1200, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "# 데이터를 train/validation/test로 나누기\n",
        "X = np.concatenate((overripe_data, no_data, mature_data, incipient_data), axis=0)\n",
        "y = np.concatenate((np.zeros(overripe_data.shape[0]), np.ones(no_data.shape[0]),\n",
        "                    2*np.ones(mature_data.shape[0]), 3*np.ones(incipient_data.shape[0])))\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC5k-MspbVG3"
      },
      "source": [
        "y one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MPyh4QsebQm_"
      },
      "outputs": [],
      "source": [
        "y_train_one_hot = to_categorical(y_train)\n",
        "y_val_one_hot = to_categorical(y_val)\n",
        "y_test_one_hot = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4O4-eZBV2wRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna\n",
        "def objective(trial, X_train, y_train, X_val, y_val):\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
        "\n",
        "    vit_model = vit.vit_b32(\n",
        "        image_size=224,\n",
        "        activation='softmax',\n",
        "        pretrained=True,\n",
        "        include_top=False,\n",
        "        pretrained_top=False,\n",
        "        classes=4\n",
        "    )\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        vit_model,\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(11, activation=tfa.activations.gelu),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(4, activation='softmax')\n",
        "    ], name='vision_transformer')\n",
        "\n",
        "    optimizer = tfa.optimizers.RectifiedAdam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=batch_size, verbose=0)\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "XR409t0r2usl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize hyperparameters with Optuna\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(lambda trial: objective(trial, X_train, y_train_one_hot, X_val, y_val_one_hot), n_trials=10)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = study.best_params\n",
        "print(\"Best parameters:\", best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YH4-EeE3T8W",
        "outputId": "e6fafa27-582d-4cfa-d341-8de2c9dd6e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-29 02:54:23,973] A new study created in memory with name: no-name-f18da2b5-e3ed-4b60-bc3e-786955d94045\n",
            "<ipython-input-15-4f8a48eab8a4>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
            "[I 2024-05-29 02:59:09,549] Trial 0 finished with value: 1.0764416456222534 and parameters: {'lr': 0.00012039666724157112, 'batch_size': 16}. Best is trial 0 with value: 1.0764416456222534.\n",
            "[I 2024-05-29 03:03:56,313] Trial 1 finished with value: 1.2050514221191406 and parameters: {'lr': 0.0003104371306175066, 'batch_size': 16}. Best is trial 0 with value: 1.0764416456222534.\n",
            "[I 2024-05-29 03:07:42,927] Trial 2 finished with value: 0.8738086223602295 and parameters: {'lr': 0.00011705756200611183, 'batch_size': 64}. Best is trial 2 with value: 0.8738086223602295.\n",
            "[I 2024-05-29 03:11:31,762] Trial 3 finished with value: 0.8943029642105103 and parameters: {'lr': 0.00012801963978427975, 'batch_size': 64}. Best is trial 2 with value: 0.8738086223602295.\n",
            "[I 2024-05-29 03:16:18,196] Trial 4 finished with value: 0.9563939571380615 and parameters: {'lr': 0.0001567834406058993, 'batch_size': 16}. Best is trial 2 with value: 0.8738086223602295.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhdB7e9DckDT"
      },
      "source": [
        "Training & Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMC5meL0UMjy"
      },
      "outputs": [],
      "source": [
        "vit_model = vit.vit_b32(\n",
        "        image_size = 224,\n",
        "        activation = 'softmax',\n",
        "        pretrained = True,\n",
        "        include_top = False,\n",
        "        pretrained_top = False,\n",
        "        classes = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYFoqkEeXQ1o"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1salmoP_XgH4"
      },
      "outputs": [],
      "source": [
        "X_train.all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6Cei5K9cgft"
      },
      "outputs": [],
      "source": [
        "#def train_vgg16_model():\n",
        "\n",
        "  #Freeze the base model layers\n",
        "  #for layer in base_model.layers:\n",
        "  #    layer.trainable = False\n",
        "\n",
        "# Add custom top layers for classification\n",
        "model = tf.keras.Sequential([\n",
        "        vit_model,\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(11, activation = tfa.activations.gelu),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(4, 'softmax')\n",
        "    ],\n",
        "    name = 'vision_transformer')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "  # Compile model\n",
        "  # optimizer = Adam(lr=5.520238899015578e-05)\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = tfa.optimizers.RectifiedAdam(learning_rate=best_params['lr'])\n",
        "\n",
        "model.compile(optimizer = optimizer,\n",
        "                loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "batch_size = best_params['batch_size']\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n",
        "                                                factor = 0.2,\n",
        "                                                patience = 2,\n",
        "                                                verbose = 1,\n",
        "                                                min_delta = 1e-4,\n",
        "                                                min_lr = 1e-6,\n",
        "                                                mode = 'max')\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
        "                                                min_delta = 1e-4,\n",
        "                                                patience = 5,\n",
        "                                                mode = 'max',\n",
        "                                                restore_best_weights = True,\n",
        "                                                verbose = 1)\n",
        "\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = './model.hdf5',\n",
        "                                                  monitor = 'val_accuracy',\n",
        "                                                  verbose = 1,\n",
        "                                                  save_best_only = True,\n",
        "                                                  save_weights_only = True,\n",
        "                                                  mode = 'max')\n",
        "\n",
        "callbacks = [earlystopping, reduce_lr, checkpointer]\n",
        "\n",
        "model.fit(x = X_train,\n",
        "          y = y_train_one_hot,\n",
        "          # steps_per_epoch = batch_size,\n",
        "          validation_data = (X_val, y_val_one_hot),\n",
        "          # validation_steps = batch_size,\n",
        "          epochs = 50,\n",
        "          callbacks = callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZbr4a9fcdg7"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test_one_hot)\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYCIbCrN0w2z"
      },
      "source": [
        "## GRAD-CAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ6vX7cxs4x3"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPTvGGYVnt3b"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/jacobgil/pytorch-grad-cam.git\n",
        "!cd pytorch-grad-cam\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WsE35zttIWi"
      },
      "outputs": [],
      "source": [
        "cd pytorch-grad-cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1k8i_AYtL7V"
      },
      "outputs": [],
      "source": [
        "!pip install -e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT1DWKwutVnx"
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEEvCGHkz53q"
      },
      "outputs": [],
      "source": [
        "pip install ttach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYR3R9untaN2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from transformers import ViTModel, ViTConfig\n",
        "import timm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McvVr3eStdeF"
      },
      "outputs": [],
      "source": [
        "def reshape_transform(tensor, height=14, width=14):\n",
        "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
        "                                      height, width, tensor.size(2))\n",
        "\n",
        "    # Bring the channels to the first dimension,\n",
        "    # like in CNNs.\n",
        "    result = result.transpose(2, 3).transpose(1, 2)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaMj_A_ytj5A"
      },
      "outputs": [],
      "source": [
        "def vit_grad_cam(image_path, class_idx):\n",
        "    # Load ViT model\n",
        "    model = torch.hub.load('facebookresearch/deit:main',\n",
        "                           'deit_tiny_patch16_224', pretrained=True)\n",
        "    model.eval()\n",
        "    target_layers = [model.blocks[-1].norm1]\n",
        "\n",
        "    rgb_img = cv2.imread(image_path, 1)[:, :, ::-1]\n",
        "    rgb_img = cv2.resize(rgb_img, (224, 224))\n",
        "    rgb_img = np.float32(rgb_img) / 255\n",
        "    input_tensor = preprocess_image(rgb_img, mean=[0.5, 0.5, 0.5],\n",
        "                                    std=[0.5, 0.5, 0.5])\n",
        "\n",
        "    cam = GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)\n",
        "    grayscale_cam = cam(input_tensor=input_tensor)\n",
        "\n",
        "    grayscale_cam = grayscale_cam[0, :]\n",
        "    cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n",
        "    return cam_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHCksxSr0IgH"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/My Drive/data/Final_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCOmE5D-0OZh"
      },
      "outputs": [],
      "source": [
        "# Set image path and target class index\n",
        "image_path = 'incipient/crop_D25_19751681-60a5-11ec-8402-0a7404972c70.png'\n",
        "image_pathes = os.listdir('./incipient/')\n",
        "print(image_pathes[:15])\n",
        "class_idx = 0\n",
        "\n",
        "# Get Grad-CAM image\n",
        "# grad_cam_image = vit_grad_cam(image_path, class_idx)\n",
        "# cv2.imwrite('grad_cam.jpg', grad_cam_image)\n",
        "\n",
        "# Display Grad-CAM image\n",
        "# img = Image.fromarray(grad_cam_image)\n",
        "# display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRA-S9zh0S8d"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    image_path = 'incipient/' + image_pathes[i]\n",
        "    class_idx = 0\n",
        "    image = cv2.imread(image_path)\n",
        "    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81LzRzFz0WOu"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    image_path = 'incipient/' + image_pathes[i]\n",
        "    class_idx = 0\n",
        "    grad_cam_image = vit_grad_cam(image_path, class_idx)\n",
        "    cv2.imwrite('grad_cam.jpg', grad_cam_image)\n",
        "    ax.imshow(grad_cam_image)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diAjpjqJ03PV"
      },
      "source": [
        "## LIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTh0qK9p07PG"
      },
      "outputs": [],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLvrVbtrDydz"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from lime import lime_image\n",
        "\n",
        "def visualize_image(image_data):\n",
        "  # LIME 설명 인스턴스 생성\n",
        "  explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "  # LIME이 필요한 이미지 하나 불러오기\n",
        "  # image = incipient_data[0]\n",
        "  image_15 = image_data[:15]\n",
        "\n",
        "  fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
        "\n",
        "  for i, ax in enumerate(axes.flatten()):\n",
        "    image = image_15[i]\n",
        "\n",
        "    # LIME 설명 생성\n",
        "    explanation = explainer.explain_instance(image, model.predict, top_labels=1, hide_color=0, num_samples=1000)\n",
        "\n",
        "    # 설명된 이미지와 설명에 대한 마스크 가져오기\n",
        "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=5, hide_rest=False)\n",
        "\n",
        "    # 설명된 이미지와 마스크를 원본 이미지에 적용\n",
        "    marked_image = mark_boundaries(temp / 2 + 0.5, mask)\n",
        "\n",
        "    ax.imshow(marked_image)\n",
        "    ax.set_title(labels[explanation.top_labels[0]])\n",
        "    ax.axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "    # # 이미지 출력\n",
        "    # plt.imshow(marked_image)\n",
        "    # plt.axis('off')\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSViiw4kDqv-"
      },
      "outputs": [],
      "source": [
        "visualize_image(incipient_data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}